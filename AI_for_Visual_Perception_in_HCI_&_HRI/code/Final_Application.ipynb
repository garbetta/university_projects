{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_zSzpbJWRwm","outputId":"f8b25c68-5a7c-49b5-d59b-8c2d4585e7f3","executionInfo":{"status":"ok","timestamp":1686054573509,"user_tz":-120,"elapsed":21178,"user":{"displayName":"Alessandro Garbetta","userId":"17876464049512819690"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"QRTCl8lWWcne"},"source":["# IMPORT MODULES"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZEXjj3Jtvk8","outputId":"78ae1ba2-85f8-4cb6-c696-88260856f4c7","executionInfo":{"status":"ok","timestamp":1686054580111,"user_tz":-120,"elapsed":6605,"user":{"displayName":"Alessandro Garbetta","userId":"17876464049512819690"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ultralytics\n","  Downloading ultralytics-8.0.114-py3-none-any.whl (595 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.4/595.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (8.4.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.27.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.65.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n","Installing collected packages: ultralytics\n","Successfully installed ultralytics-8.0.114\n"]}],"source":["#PIP INSTALL\n","!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbn9tYeBW8Hi"},"outputs":[],"source":["import os\n","import shutil\n","import re\n","import cv2\n","import csv\n","import moviepy.editor\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from time import sleep\n","from google.colab.patches import cv2_imshow\n","\n","from ultralytics import YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQ_R2cIMqHoG"},"outputs":[],"source":["from pathlib import Path\n","from PIL import Image\n","import torch\n","from torch import flatten\n","\n","import torch.nn as nn\n","from torch.nn import Module, Conv2d, Linear, MaxPool2d, ReLU, LogSoftmax, Dropout\n","\n","import torchvision.transforms.functional as TF\n","from torchvision import models\n","import torch.utils.data as data_utils\n","from torch.utils.data import Dataset, random_split, DataLoader\n","from torch.optim import Adam\n","from torchvision.datasets import KMNIST\n","#from torchmetrics.functional import accuracy\n","from torch.nn.utils.rnn import pad_sequence\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor\n","import argparse\n","import imutils\n","import time\n","import dlib\n","from sklearn.preprocessing import normalize\n","import math\n","import sys\n","\n","ROOT = '/code/'\n","GENERAL_DIR = ROOT + 'FINAL_TEST/'\n","FRAMES  = GENERAL_DIR + 'FRAMES/'\n","\n","sys.path.append(ROOT+'mtcnn')#'/content/drive/MyDrive/NAPOLI/mtcnn/') #ROOT+mtcnn\n","from mtcnn import MTCNN\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"RTG5kWkOXA2w"},"source":["# FUNCTIONS / CLASSES"]},{"cell_type":"markdown","metadata":{"id":"mEE0trxtpS2y"},"source":["## **FUNCTIONS EXTERNAL PART**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMLIsXx03Z2L"},"outputs":[],"source":["# FUNCTION TO REMOVE IMAGES AND LABELS FILE WITHOUT DETECTIONS\n","def YOLO_rm_empty_file(path, save_dir):\n","  flag = 0\n","  for each in os.listdir(path+\"labels\"):   \n","    f = open(path+\"labels/\"+each, \"r\")\n","    \n","    if f.read() == '':\n","      os.remove(path+\"labels/\"+each)            #remove file .txt empty\n","      #os.remove(path+each[:-4]+\".png\")         #remove images without relevant traffic signs\n","      #if read file one to one\n","      os.rmdir(path+\"labels/\")\n","      os.rmdir(path)\n","      flag = 0\n","      ##########\n","    else:\n","      shutil.move(path+\"labels/\"+each, save_dir)\n","      #shutil.move(path+each[:-4]+\".png\", save_dir)\n","      os.rmdir(path+\"labels/\")\n","      os.rmdir(path)\n","      flag = 1\n","    f.close()\n","  return flag\n","\n","# FUNCTION TO DEVIDE 30 SECOND CLIP IN FRAMES\n","def getFrames_30clip(vid, output, rate, frameName, flag):   \n","    vidcap = cv2.VideoCapture(vid)\n","    clip = moviepy.editor.VideoFileClip(vid)\n","\n","    seconds = clip.duration\n","    print('duration: ' + str(seconds))\n","    \n","    count = 0\n","    frame = 0\n","    \n","    if not os.path.isdir(output):\n","        os.mkdir(output)\n","    \n","    success = True\n","    while success:\n","        vidcap.set(cv2.CAP_PROP_POS_MSEC,frame*1000)      \n","        success, image = vidcap.read()\n","\n","        ## Stop when last frame is identified\n","        if frame > seconds or not success:\n","            break\n","\n","        print('extracting frame ' + frameName + '-%d.png' % count)\n","        name = output + '/' + frameName + '-%d.png' % count # save frame as PNG file\n","        #################\n","        if int(flag) == 0: crop_img = image\n","        else: crop_img = image[:,240:1680,:] \n","        ################\n","        cv2.imwrite(name, crop_img)\n","        frame += rate\n","        count += 1\n","\n","#FUNCTION TO EXTRACT THE INFO FROM THE INFO OF TRAFFIC SIGNS' DETECTIONS \n","def extraction_from_txt_label(txt_path, width, height):\n","\n","  f = open(txt_path)\n","\n","  read = f.read()\n","  #print(read)\n","  token = re.split(\" |\\n\", read)\n","  token = token[:-1]\n","  #print(token)\n","  size = len(token)\n","  #print(size)\n","  #to normalize values\n","  info_points = []\n","  i = 0\n","  while i < (size):\n","    i += 1\n","    center_x = float(token[i]) * width\n","    i += 1\n","    center_y = float(token[i]) * height\n","    i += 1\n","    box_x = float(token[i]) * width\n","    i += 1\n","    box_y = float(token[i]) * height\n","    i += 1\n","    info_points.append((center_x, center_y, box_x, box_y))\n","\n","  return info_points, size//5\n","\n","# VARIABLES FOR THE VERTICES\n","def vertex_box(center_x, center_y, box_x, box_y):\n","  s_s = ( (center_x-(box_x/2)) , (center_y-(box_y/2)) ) #left upper\n","  i_s = ( (center_x-(box_x/2)) , (center_y+(box_y/2)) ) #left lower\n","  s_d = ( (center_x+(box_x/2)) , (center_y-(box_y/2)) ) #right upper \n","  i_d = ( (center_x+(box_x/2)) , (center_y+(box_y/2)) ) #right lowers\n","\n","  return s_s, i_s, s_d, i_d\n","\n","# VALUES OF THE GRID ON THE EXTERNAL IMAGES\n","def grid(width, height):\n","  #names of variables with positions of grid, x and y axis\n","  #z -> zero\n","  #u -> uno\n","  #d -> due \n","  #t -> tre\n","\n","  z_x = 0\n","  z_y = 0\n","\n","  u_x = width/3       #480\n","  d_x = (2/3)*width   #960\n","  t_x = width         #1440\n","\n","  u_y = height/3      #360\n","  d_y = (2/3)*height  #720\n","  t_y = height        #1080\n","\n","  return z_x, z_y, u_x, d_x, t_x, u_y, d_y, t_y\n","\n","# FUNCTIONS TO INDICATE THE POSITION AND CENTER OF EACH SING\n","def detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, x1_x, x2_x, y1_y, y2_y, str1):\n","  str_ret = ''\n","  if x1_x < XC < x2_x  and y1_y < YC < y2_y :  \n","    #print(\"CENTER IS IN CELL \", str1)\n","    str_ret += 'CT'+str1\n","  #else : str_ret += ' '\n","  if x1_x < s_s[0] < x2_x  and y1_y < s_s[1] < y2_y :  \n","    #print(\"UPPER LEFT END IS IN CELL \", str1)\n","    str_ret += 'UL'+str1\n","  #else : str_ret += ' '\n","  if x1_x < i_s[0] < x2_x  and y1_y < i_s[1] < y2_y :  \n","    #print(\"LOWER LEFT END IS IN \", str1) \n","    str_ret += 'LL'+str1\n","  #else : str_ret += ' '\n","  if x1_x < s_d[0] < x2_x  and y1_y < s_d[1] < y2_y :  \n","    #print(\"UPPER RIGHT END IS IN CELL \", str1)\n","    str_ret += 'UR'+str1\n","  #else : str_ret += ' '\n","  if x1_x < i_d[0] < x2_x  and y1_y < i_d[1] < y2_y :  \n","    #print(\"LOWER RIGHT END IS IN CELL \", str1)\n","    str_ret += 'LR'+str1\n","  #else : str_ret += ' '\n","\n","  return str_ret \n","\n","# FUNCTION FOR EACH CELL, RETURN STRING WITH THE CELL WHERE IS THE SIGNAL\n","def final_cell(XC, YC, s_s, i_s, s_d, i_d, z_x, u_x, d_x, t_x, z_y, u_y, d_y, t_y):\n","  \n","  str_ret = ''  \n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, z_x, u_x, z_y, u_y, \"1\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, u_x, d_x, z_y, u_y, \"2\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, d_x, t_x, z_y, u_y, \"3\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, z_x, u_x, u_y, d_y, \"4\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, u_x, d_x, u_y, d_y, \"5\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, d_x, t_x, u_y, d_y, \"6\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, z_x, u_x, d_y, t_y, \"7\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, u_x, d_x, d_y, t_y, \"8\")\n","  str_ret += detect_center_vertex(XC, YC, s_s, i_s, s_d, i_d, d_x, t_x, d_y, t_y, \"9\")\n","\n","  return str_ret\n","\n","# FUNCTION TO DRAW GRID\n","def draw_rect(image, z_x, u_x, d_x, t_x, z_y, u_y, d_y, t_y, dest_img_path):\n","  #PARAMEERS: images, lower upper corner, right lower corner, color, thickness\n","\n","  cv2.rectangle(image, (int(z_x), int(z_y)), (int(u_x), int(u_y)), (0, 255, 0), 5)        #1_1\n","  cv2.rectangle(image, (int(u_x), int(z_y)), (int(d_x), int(u_y)), (0, 255, 0), 5)        #1_2\n","  cv2.rectangle(image, (int(d_x), int(z_y)), (int(t_x), int(u_y)), (0, 255, 0), 5)        #1_3\n","  cv2.rectangle(image, (int(z_x), int(u_y)), (int(u_x), int(d_y)), (0, 255, 0), 5)        #2_1\n","  cv2.rectangle(image, (int(u_x), int(u_y)), (int(d_x), int(d_y)), (0, 255, 0), 5)        #2_2    \n","  cv2.rectangle(image, (int(d_x), int(u_y)), (int(t_x), int(d_y)), (0, 255, 0), 5)        #2_3\n","  cv2.rectangle(image, (int(z_x), int(d_y)), (int(u_x), int(t_y)), (0, 255, 0), 5)        #3_1   \n","  cv2.rectangle(image, (int(u_x), int(d_y)), (int(d_x), int(t_y)), (0, 255, 0), 5)        #3_2\n","  cv2.rectangle(image, (int(d_x), int(d_y)), (int(t_x), int(t_y)), (0, 255, 0), 5)        #3_3      \n","  \n","  desired_width = 500\n","\n","  height, width, _ = image.shape\n","  aspect_ratio = width / height\n","  new_height = int(desired_width / aspect_ratio)\n","\n","  resized_img = cv2.resize(image, (desired_width, new_height))\n","\n","  #image = np.resize(image, 640)\n","\n","  cv2_imshow(resized_img)\n","  if dest_img_path != ' ': cv2.imwrite(dest_img_path, image)\n","\n","# GENERAL FUNCTION, INPUT IMAGE AND TXT, RETURN THE INFO OF THE POSITION\n","def image_test_cell(image_path, file_txt, dest_img_path):\n","  \n","  image = cv2.imread(image_path)\n","  height, width, c = image.shape\n","  #print(height, width)\n","\n","  info_points, n_elem = extraction_from_txt_label(file_txt, width, height)\n","  ret_celle_string = []\n","  \n","  for i in range(n_elem):\n","    center_x = info_points[i][0]\n","    center_y = info_points[i][1]\n","    box_x = info_points[i][2]\n","    box_y = info_points[i][3]\n","    #print del cartello detected from YOLO\n","    j = i+1\n","    #print(f\"{j}° Element detected in photo\")\n","    cv2.rectangle(image, (int(center_x-(box_x/2)), int(center_y-(box_y/2))), (int(center_x+(box_x/2)), int(center_y+(box_y/2))), (255, 0, 0), 5)      \n","    cv2.circle(image, (int(center_x), int(center_y)), 5, (0,0,255), 5)\n","\n","    #ESTREMI BOX SIGN\n","    s_s, i_s, s_d, i_d = vertex_box(center_x, center_y, box_x, box_y)\n","\n","    z_x, z_y, u_x, d_x, t_x, u_y, d_y, t_y = grid(width, height)\n","\n","    str_ret = final_cell(center_x, center_y, s_s, i_s, s_d, i_d, z_x, u_x, d_x, t_x, z_y, u_y, d_y, t_y)\n","    \n","    #ret_celle_string += str_ret+' '\n","    ret_celle_string.append(str_ret)\n","\n","  #print(ret_celle_string)\n","\n","  draw_rect(image, z_x, u_x, d_x, t_x, z_y, u_y, d_y, t_y, dest_img_path)\n","\n","  return ret_celle_string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFkGV1bu1I6F"},"outputs":[],"source":["#list without repetitions of signs' positions \n","def uniques_pos(positions):\n","  list_pos = []\n","  for elem in positions:\n","    i = 0\n","    list_pos_elem = set()\n","    while i+2 < len(elem):  \n","      list_pos_elem.add(elem[i+2])\n","      i = i+3\n","    list_pos.append(list_pos_elem)\n","  \n","  return list_pos \n","\n","#number of signs seen\n","def cell_and_signs(predicted_cell, list_uniques_pos):\n","  c = 0\n","  for elem in list_uniques_pos:\n","    #print(elem)\n","    if str(predicted_cell) in elem:\n","      c += 1\n","\n","  return c"]},{"cell_type":"markdown","metadata":{"id":"d-pAApWVpjUm"},"source":["## ****FUNCTIONS INTERNAL PART****"]},{"cell_type":"markdown","metadata":{"id":"U2s-tZdQrz1e"},"source":["**Head pose estimation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YacKmuq1Ktgu"},"outputs":[],"source":["class PoseEstimator:\n","    def __init__(self, weights=None):\n","      self.detector =  MTCNN()\n","\n","    def detect_faces(self, image, image_shape_max=640):\n","      '''\n","      Parameters\n","      ----------\n","      image : uint8 image for face detection.\n","      image_shape_max : int, optional maximum size (in pixels) of image. The default is None.\n","\n","      Returns\n","      -------\n","      float array bounding boxes and score.\n","      float array landmarks.\n","\n","      '''\n","      image_shape = image.shape[:2]\n","      \n","      # perform image resize for faster detection    \n","      if image_shape_max:\n","          scale_factor = max([1, max(image_shape) / image_shape_max])\n","      else:\n","          scale_factor = 1\n","          \n","      if scale_factor > 1:        \n","          scaled_image = cv2.resize(image, (0, 0), fx = 1/scale_factor, fy = 1/scale_factor)\n","          \n","          start = time.time()\n","          print(\"[INFO[ performing face detection with MTCNN...\")\n","          boxes, points = self.detector.detect_faces(scaled_image)\n","          end = time.time()\n","          print(\"[INFO] face detection took {:.4f} seconds\".format(end - start))\n","\n","          boxes[:,:4] *= scale_factor\n","          points *= scale_factor\n","\n","      else:\n","          start = time.time()\n","          print(\"[INFO[ performing face detection with MTCNN...\")\n","          boxes, points = self.detector.detect_faces(image)\n","          end = time.time()\n","          print(\"[INFO] face detection took {:.4f} seconds\".format(end - start))\n","      \n","      return boxes, points\n","\n","    def draw_landmarks(self, image, boxes, points):\n","      '''\n","      Parameters\n","      ----------\n","      frame : TYPE RGB image\n","      bb : TYPE - Array of float64, Size = (5,) coordinates of bounding box for the selected face.\n","      points : TYPE - Array of float32, Size = (10,) coordinates of landmarks for the selected faces.\n","\n","      Returns\n","      -------\n","      None.\n","\n","      '''\n","      font = cv2.FONT_HERSHEY_COMPLEX # Text in video\n","      font_size = 0.6\n","      blue = (0, 0, 255)\n","      green = (0,128,0)\n","      red = (255, 0, 0)\n","\n","      boxes = boxes.astype(int)\n","      points = points.astype(int)\n","      # draw rectangle and landmarks on face\n","      cv2.rectangle(image, (boxes[0], boxes[1]), (boxes[2], boxes[3]), red, 1)\n","      cv2.circle(image, (int(points[0]), int(points[5])), 2, blue, 2)# left eye\n","      cv2.circle(image, (int(points[1]), int(points[6])), 2, blue, 2)# right eye\n","      cv2.circle(image, (int(points[2]), int(points[7])), 2, blue, 2)# nose\n","      cv2.circle(image, (int(points[3]), int(points[8])), 2, blue, 2)# mouth - left\n","      cv2.circle(image, (int(points[4]), int(points[9])), 2, blue, 2)# mouth - right \n","      \n","      cv2_imshow(image)\n","      \n","      #w = int(boxes[2])-int(boxes[0]) # width\n","      #h = int(boxes[3])-int(boxes[1]) # height\n","      \n","\n","    def one_face(self, frame, bbs, pointss):\n","      \"\"\"\n","      Parameters\n","      ----------\n","      frame : TYPE RGB image (numpy array).\n","      bbs : TYPE - Array of flaot64, Size = (N, 5) coordinates of bounding boxes for all detected faces.\n","      pointss : TYPE - Array of flaot32, Size = (N, 10) coordinates of landmarks for all detected faces.\n","      Returns\n","      -------\n","      bb : TYPE - Array of float 64, Size = (5,) coordinates of bounding box for the selected face.\n","      points : TYPE coordinates of five landmarks for the selected face.\n","      \"\"\"\n","      # select only process only one face (center)\n","      offsets = [(bbs[:,0]+bbs[:,2])/2-frame.shape[1]/2,\n","                (bbs[:,1]+bbs[:,3])/2-frame.shape[0]/2]\n","      offset_dist = np.sum(np.abs(offsets),0)\n","      index = np.argmin(offset_dist)\n","      bb = bbs[index]\n","      points = pointss[:,index]\n","      return bb, points\n","\n","\n","    def find_roll(self, points):\n","      \"\"\"\n","      Parameters\n","      ----------\n","      points : TYPE - Array of float32, Size = (10,) coordinates of landmarks for the selected faces.\n","      Returns\n","      -------\n","      roll of face.\n","\n","      \"\"\"\n","      return points[6] - points[5]\n","\n","    def find_yaw(self, points):\n","      \"\"\"\n","      Parameters\n","      ----------\n","      points : TYPE - Array of float32, Size = (10,) coordinates of landmarks for the selected faces.\n","      Returns\n","      -------\n","      yaw of face.\n","\n","      \"\"\"\n","      le2n = points[2] - points[0]\n","      re2n = points[1] - points[2]\n","      return le2n - re2n\n","\n","    def find_pitch(self, points):\n","      \"\"\"\n","      Parameters\n","      ----------\n","      points : TYPE - Array of float32, Size = (10,) coordinates of landmarks for the selected faces.\n","      Returns\n","      -------\n","      Pitch\n","      \"\"\"\n","      eye_y = (points[5] + points[6]) / 2\n","      mou_y = (points[8] + points[9]) / 2\n","      e2n = eye_y - points[7]\n","      n2m = points[7] - mou_y\n","      return e2n / n2m\n"]},{"cell_type":"markdown","metadata":{"id":"QxpHEuvDryGe"},"source":["**Process Data for classifier**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHGh1n4x82fY"},"outputs":[],"source":["class processData:\n","\n","  def __init__(self, model):\n","    self.model = model\n","    \n","  def extract_features_from_path(self, path, dest_img_path):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    path : path of an image\n","    dest_img_path : destination path of the image (after cropping)\n","\n","    Returns\n","    -------\n","    face_crop : cropped image (with only the face)\n","    features : [roll, pitch, yaw, lx, ly, rx, ry] head pose angles + coordinates of the two pupils\n","    \"\"\"\n","    #inizialize the pose estimator\n","    est = PoseEstimator()\n","\n","    image = cv2.imread(path)\n","    image = imutils.resize(image, width=600)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # convert to rgb\n","    image_rgb = cv2.flip(image_rgb, 1)                  # flip for user friendliness\n","  \n","    bounding_boxes, landmarks = est.detect_faces(image_rgb)\n","\n","    #print(bounding_boxes)\n","\n","    if len(bounding_boxes) > 0:\n","      if len(bounding_boxes) >= 2:\n","        bounding_boxes, landmarks = est.one_face(image_rgb, bounding_boxes, landmarks)       \n","        roll = est.find_roll(landmarks)\n","        pitch = est.find_pitch(landmarks)\n","        yaw = est.find_yaw(landmarks)\n","        \n","        lx = landmarks[0]\n","        ly = landmarks[5]\n","        rx = landmarks[1]\n","        ry = landmarks[6]\n","        \n","        bounding_boxes = bounding_boxes.astype(int)\n","        # draw rectangle on face\n","        x1 = bounding_boxes[0]\n","        y1 = bounding_boxes[1]\n","        x2 = bounding_boxes[2]\n","        y2 = bounding_boxes[3]\n","        img = cv2.flip(image, 1)\n","        #cv2.rectangle(img, (bounding_boxes[0], bounding_boxes[1]), (bounding_boxes[2], bounding_boxes[3]), red, 1)\n","        #cv2_imshow(img)\n","\n","        w = bounding_boxes[2] - bounding_boxes[0]\n","        h = bounding_boxes[3] - bounding_boxes[1]\n","        max_s = max(w, h)\n","        \n","        diff_x = (max_s - w)/2\n","        diff_y = (max_s - h)/2\n","        \n","        #to cut the original immages, using face's box\n","        face_crop = img[int(y1-diff_y):int(y2+diff_y), int(x1-diff_x):int(x2+diff_x)]\n","\n","        cv2_imshow(face_crop)   \n","        if dest_img_path != ' ': cv2.imwrite(dest_img_path, face_crop)\n","\n","      else:\n","        roll = est.find_roll(landmarks)\n","        pitch = est.find_pitch(landmarks)\n","        yaw = est.find_yaw(landmarks)\n","        lx = landmarks[0]\n","        ly = landmarks[5]\n","        rx = landmarks[1]\n","        ry = landmarks[6]\n","\n","        bounding_boxes = bounding_boxes.astype(int)\n","        # draw rectangle on face\n","        x1 = bounding_boxes[0][0]\n","        y1 = bounding_boxes[0][1]\n","        x2 = bounding_boxes[0][2]\n","        y2 = bounding_boxes[0][3]\n","        img = cv2.flip(image, 1)\n","        #cv2.rectangle(img, (bounding_boxes[0][0], bounding_boxes[0][1]), (bounding_boxes[0][2], bounding_boxes[0][3]), red, 1)\n","        #cv2_imshow(img)\n","\n","        w = bounding_boxes[0][2] - bounding_boxes[0][0]\n","        h = bounding_boxes[0][3] - bounding_boxes[0][1]\n","        max_s = max(w, h)\n","        \n","        diff_x = (max_s - w)/2\n","        diff_y = (max_s - h)/2\n","        \n","        #to cut the original immages, using face's box\n","        face_crop = img[int(y1-diff_y):int(y2+diff_y), int(x1-diff_x):int(x2+diff_x)]\n","\n","        cv2_imshow(face_crop)\n","        if dest_img_path != ' ': cv2.imwrite(dest_img_path, face_crop)\n","\n","    else:\n","      print(\"NO FACE FOUND - NEXT EXTERNAL FRAME\")\n","      return [], []\n","\n","    #FEATURES for NN (head pose, pupils) \n","\n","    features = [float(roll), float(pitch), float(yaw), float(lx), float(ly), float(rx), float(ry)]\n","\n","    return face_crop, features\n","\n","\n","  def preprocess_for_NN(self, image, features):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    image : cropped image of the face\n","    features : [roll, pitch, yaw, lx, ly, rx, ry] head pose angles + coordinates of the two pupils\n","\n","    Returns\n","    -------\n","    sample : torch tensor that contains the conatenation of image tensor + features  [dim(3, 224, 231)]\n","    \"\"\"\n","    preprocess = transforms.Compose([\n","                              transforms.ToPILImage(),\n","                              transforms.Resize(224),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","                              ])\n","\n","    image_tensor = preprocess(image)\n","\n","    features_tensor = torch.tensor(features).squeeze(-1)\n","    \n","    feature = features_tensor.repeat(3, 224, 1)#.squeeze(-1)\n","    #print(image_tensor.shape, features_tensor.shape)\n","\n","    sample = torch.cat((image_tensor, feature), 2)  \n","\n","    #print(sample)\n","    #print(len(sample))\n","\n","    return sample\n","\n","\n","  def gaze_prediction(self, sample, device):\n","    \"\"\"\n","    Cell prediction \n","\n","    Parameters\n","    ----------\n","    sample : torch tensor that contains the conatenation of image tensor + features  [dim(3, 224, 231)]\n","    device : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    Returns\n","    -------\n","    gaze_cell : number of the predicted cell\n","    \"\"\"\n","    totalTestLoss = 0\n","\n","    with torch.no_grad():\n","\n","      x = sample.to(dtype=torch.float32).unsqueeze(0)\n","      #print(x.shape)\n","\n","      # send the input to the device\n","      x = x.to(device)\n","      # make the predictions and calculate the validation loss\n","      pred = self.model(x)\n","\n","      # calculate the number of correct predictions\n","      gaze_cell = int(pred.argmax(1))+1\n","      if gaze_cell == 10 : gaze_cell = 0\n","  \n","    return gaze_cell"]},{"cell_type":"markdown","metadata":{"id":"iln_vh0w3shF"},"source":["## **Classification Network**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0ner9O_GbV3"},"outputs":[],"source":["class HEGClass(Module):\n","  \n","  def __init__(self, numChannels, classes):\n","\t\t# call the parent constructor\n","\n","    super(HEGClass, self).__init__()  #input(32, 3, 224, 231) \n","\n","    self.vgg16 = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n","    self.vgg16_featuresextractor = nn.Sequential(*list(self.vgg16.children())[:-1]).to('cuda')\n","\n","    self.fc6 = Linear(in_features=25088+7, out_features=4096)\n","    self.relu6_1 = ReLU()\n","    self.drop6 = Dropout(0.5)\n","\n","    self.fc7 = Linear(in_features=4096, out_features=1000)\n","    self.relu7_1 = ReLU()\n","    self.drop7 = Dropout(0.5)\n","\n","    self.fc8 = Linear(in_features=1000, out_features=classes)\n","    self.logSoftmax = LogSoftmax(dim=1)\n","  \n","  def forward(self, s): #s.shape = 5x1x224x231\n","\n","    #print(s.shape)\n","    x, feature = torch.split(s, [224, 7], dim=3)\n","    #print(x.shape)\n","    #print(feature.shape)\n","\n","    features_images = self.vgg16_featuresextractor(x) \n","    \n","    #print(features_images.shape)\n","    features_images = flatten(features_images, 1)\n","    #print(features_images.shape)\n","\n","    feature = feature[:, 0, 0, :]    #torch.Size([32, 7])\n","    #print(feature.shape)\n","    \n","    x = torch.cat((features_images, feature), 1)\n","    x = self.fc6(x)\n","    x = self.relu6_1(x)\n","    x = self.drop6(x)\n","    x = self.fc7(x)\n","    x = self.relu7_1(x)\n","    x = self.drop7(x)\n","\n","    x = self.fc8(x)\n","    output = self.logSoftmax(x)\n","\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"XsvH8z5EJJ_k"},"source":["## **Functions for csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMyjezIvGPgg"},"outputs":[],"source":["def init_csv_prediction(path_csv):\n","  header = []\n","\n","  header.append(\"n° of frames\")     #file's name - with number of frames\n","  header.append(\"n° of signs\")      #number of signs present in all the image \n","  header.append(\"signs position\")   #uniques number where are center and angles of the images\n","  header.append(\"gaze prediction\")  #the value of the network\n","  header.append(\"n° signs seen\")    #if i see those cell how many signs see? number of signs present in the cell that i seeing\n","  header.append(\"Attention\")        #boolean  \n","\n","  \n","  with open(path_csv, 'w') as csv_f:\n","    writer = csv.writer(csv_f)\n","    writer.writerow(header) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZyi7aFP_Y44"},"outputs":[],"source":["def write_line_csv(file, csv_path, positions, predicted_cell, counter):\n","    \n","    set_pos = uniques_pos(positions)\n","\n","    #number of signs seen when you see one cell\n","    num_signs = cell_and_signs(predicted_cell, set_pos)\n","\n","    if num_signs >= 1 or (predicted_cell in [4, 5, 6]):\n","      att = True\n","      counter += 1\n","    elif predicted_cell in [7, 8]: att = 'infotainment'\n","    else: att = False\n","\n","\n","    row_csv = [file[9:-4], len(positions), set_pos, predicted_cell, num_signs, att]\n","    \n","    print(row_csv)\n","    \n","    with open(csv_path, \"a\") as csv_f:\n","      #next(csv_f)\n","      writer = csv.writer(csv_f)\n","      writer.writerow(row_csv)\n","      \n","      csv_f.close()\n","    return counter\n"]},{"cell_type":"markdown","metadata":{"id":"Q1ZaEuWLWicu"},"source":["# **EXECUTION PART**"]},{"cell_type":"markdown","metadata":{"id":"t0jpg_pzXlDs"},"source":["We take in exam an external and internal video of duration of 30 seconds. \n","\n","The user can put in *dest_dir_ext* and *dest_dir_ext* any video of any duration, the only important thing is that the 2 videos have to be sinchronized.\n","\n","If the user want to preprocess his video (make it of 30 seconds), he can find the function in UTILS.ipynb in section 30_SEC SUBCLIPS CUT."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sV4UFNXGacaT"},"outputs":[],"source":["source_clip_ext = ROOT + 'dataset/DRIVERS-on-DRIVE/EXTERNAL/'  #PATH NAME_CLIP\n","source_clip_in = ROOT + 'dataset/DRIVERS-on-DRIVE/INTERNAL/'  #PATH NAME_CLIP\n","\n","dest_dir_ext = FRAMES + 'EXTERNAL/'   #\n","dest_dir_in = FRAMES + 'INTERNAL/'    #\n","\n","getFrames_30clip(source_clip_ext, dest_dir_ext, 0.3333, 'external', 1)     \n","getFrames_30clip(source_clip_in, dest_dir_in, 0.3333, 'internal', 0)     "]},{"cell_type":"markdown","metadata":{"id":"XXtQ5B03aYK8"},"source":["## Prediction of traffic signs with YOLO v8 and gaze_pose with MTCNN + HEGClass"]},{"cell_type":"markdown","source":["Initialization of models, weights and saving directory/files paths"],"metadata":{"id":"TLojW5RrJMjN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcESrpFmF43B"},"outputs":[],"source":["YOLO_weights = ROOT + 'yolo_8_3_classes_best.pt'\n","\n","HEGClass_weights = ROOT + 'weights_vgg_pretrained_10classes_1002img(5epochs,32batch).pt'\n","\n","YOLO_model = YOLO(YOLO_weights)  # load a pretrained model \n","\n","HEGClass_model = HEGClass(numChannels=3, classes=10).to(device)\n","HEGClass_model.load_state_dict(torch.load(HEGClass_weights))\n","HEGClass_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GG1mWYt-PoTS"},"outputs":[],"source":["list_file = dest_dir_ext  #directory with frames external clip\n","save_dir = GENERAL_DIR + \"YOLO_Gaze_detection/\"\n","\n","path_csv = GENERAL_DIR + '/ANNOTATIONS/'+ 'evaluation.csv'\n","#inizializzare writer per csv\n","init_csv_prediction(path_csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NB14plROgdM7"},"outputs":[],"source":["head_gaze_pose = processData(HEGClass_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWa-WX1XXkDr","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1mEVDXhgrAkF7B1apZMi18bCE9Abr8QQ7"},"executionInfo":{"status":"ok","timestamp":1686054840050,"user_tz":-120,"elapsed":203435,"user":{"displayName":"Alessandro Garbetta","userId":"17876464049512819690"}},"outputId":"9bb52c7d-48f7-4865-bbc3-9e15573c44fe"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["n_of_att = 0\n","for file in os.listdir(list_file):\n","  #print(list_file+file)\n","  results = YOLO_model.predict(source= list_file+file, save=False, save_txt=True, conf = 0.3)   #decide the value of confidence\n","  ret = YOLO_rm_empty_file(\"/content/runs/detect/predict/\", save_dir)\n","  \n","  if ret == 1:\n","    image_ext_path = list_file+file  \n","    print(image_ext_path)\n","    yolo_txt_path = save_dir+file[:-3]+'txt'\n","\n","    positions = image_test_cell(image_ext_path, yolo_txt_path, ' ')  #save_dir+file #if third parameter is '' the images with grid is not saved\n","    #len(positions) \n","    print(dest_dir_in)\n","    image_int_path = dest_dir_in+'internal'+file[8:] \n","    print(image_int_path)\n","\n","    image, features = head_gaze_pose.extract_features_from_path(image_int_path, ' ')  #save_dir+'internal'+file[8:] #if second parameter is '' the images with grid is not saved\n","    \n","    if len(image) == 0:\n","      print('FACE NOT RECOGNIZED')\n","      continue\n","\n","    sample = head_gaze_pose.preprocess_for_NN(image, features)\n","    \n","    #print(sample)\n","    \n","    predicted_cell = head_gaze_pose.gaze_prediction(sample, device)\n","    \n","    #print('PREDICTED CELL IS :', predicted_cell)\n","\n","    n_of_att = write_line_csv(file, path_csv, positions, predicted_cell, n_of_att)\n","\n","#print('VALORE DI MATCH: ', n_of_att)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTaIrV1bl1_u"},"outputs":[],"source":["#to execute before to evaluate other subclips\n","for elem in os.listdir(save_dir):\n","  os.remove(save_dir+elem)\n","\n","!rm -r /content/runs/detect/predict/labels\n","!rm -r /content/runs/detect/predict"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["mEE0trxtpS2y","d-pAApWVpjUm","iln_vh0w3shF","XsvH8z5EJJ_k"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}